{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Feature Engineering with Spark\n",
    "\n",
    "Problem: In `Feature Engineering`, we developed a pipeline for automated feature engineering using a dataset of customer transactions and label times. Running this pipeline on a single partition of customers takes about 15 minutes which means computing all of the features would require several days if done one at a time. \n",
    "\n",
    "Solution: Break the dataset into independent partitions of customers and run multiple subsets in parallel. This can be done using multiple processors on a single machine or a cluster of machines.\n",
    "\n",
    "## Spark with PySpark\n",
    "\n",
    "[Apache Spark](http://spark.apache.org) is a popular framework for distributed computed and large-data processing. It allows us to run computations in parallel either on a single machine, or distributed across a cluster of machines. In this notebook, we will run automated feature engineering in [Featuretools](https://github.com/Featuretools/featuretools) using Spark with the [PySpark library](http://spark.apache.org/docs/2.2.0/api/python/pyspark.html). \n",
    "\n",
    "The first step is initializing Spark. We can use the `findspark` library to make sure that `pyspark` can find Spark in the Jupyter Notebook. This notebook assumes the Spark cluster is already running. To get started with a Spark cluster, refer to [this guide](https://data-flair.training/blogs/install-apache-spark-multi-node-cluster/). \n",
    "\n",
    "(We'll skip the Featuretools details in this notebook, but for an introduction see [this article](https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219). For a comparison of manual to automated feature engineering, see [this article](https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96). )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# update based on your installation\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark \n",
    "\n",
    "A `SparkContext` is the interface to a running Spark cluster. We pass in a number of parameters to the `SparkContext` using a `SparkConf` object. Namely, we'll turn on logging, tell Spark to use 12 cores on our machine, and direct Spark to the location of the master (parent) node. \n",
    "\n",
    "Adjust the parameters depending on your cluster set up. I found [this guide](https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html) to be helpful in choosing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('spark.eventLog.enabled', 'True'), ('spark.eventLog.dir', './data/tmp/'), ('spark.num.executors', '1'), ('spark.executor.memory', '12g'), ('spark.executor.cores', '12'), ('spark.master', 'spark://AMB-R09BLVCJ:7077')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# update based on your installation\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Enable logging\n",
    "conf.set('spark.eventLog.enabled', True);\n",
    "conf.set('spark.eventLog.dir', './data/tmp/');\n",
    "\n",
    "# Use all cores on all machines\n",
    "conf.set('spark.num.executors', 1)\n",
    "conf.set('spark.executor.memory', '12g')\n",
    "conf.set('spark.executor.cores', 12)\n",
    "\n",
    "# Set the parent\n",
    "conf.set('spark.master', 'spark://AMB-R09BLVCJ:7077')\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Spark \n",
    "\n",
    "Before we get to the feature engineering, we want to test if our cluster is running correctly. We'll instantiate a `Spark` cluster and run a simple program that calculates the value of pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/09 16:33:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.77:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://AMB-R09BLVCJ:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pi_calc</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://AMB-R09BLVCJ:7077 appName=pi_calc>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"pi_calc\", \n",
    "                          conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14170524\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100000000\n",
    "import random\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "# Parallelize counting samples inside circle using Spark\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Dashboards\n",
    "\n",
    "After starting the Spark cluster  from the command line- before running any of the code in the notebook - you can view a dashboard of the cluster at localhost:8080. This shows basic information such as the number of workers and the currently running or completed jobs.\n",
    "\n",
    "\n",
    "![](../images/spark_cluster_main.png)\n",
    "\n",
    "Once a `SparkContext` has been initialized, the job can be viewed at localhost:4040. This shows particular details such as the number of tasks completed and the directed acyclic graph of the operation. \n",
    "\n",
    "![](../images/stages.png)\n",
    "\n",
    "Using the web dashboard can be a helpful method to help debug your cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cluster is running correctly, we can move on to feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Storage\n",
    "\n",
    "In previous notebooks, we partitioned our data and created feature matrices for the first 50 partitions. Normally, all of the reading and writing for running with Spark would happen through S3, but for this example we will use our local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nate.parsons/dev/open_source_demos/env/lib/python3.8/site-packages/statsmodels/compat/pandas.py:65: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import Int64Index as NumericIndex\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import featuretools as ft\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "partition = 20\n",
    "CWD = os.getcwd()\n",
    "directory = f'{CWD}/data/partitions/p' + str(partition)\n",
    "cutoff_times_file = 'MS-31_labels.csv'\n",
    "\n",
    "\n",
    "# Read in the data files\n",
    "members = pd.read_csv(f'{directory}/members.csv', \n",
    "                  parse_dates=['registration_init_time'], \n",
    "                  infer_datetime_format = True, \n",
    "                  dtype = {'gender': 'category'})\n",
    "\n",
    "trans = pd.read_csv(f'{directory}/transactions.csv',\n",
    "                   parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                    infer_datetime_format = True)\n",
    "\n",
    "logs = pd.read_csv(f'{directory}/logs.csv', parse_dates = ['date'])\n",
    "\n",
    "cutoff_times = pd.read_csv(f'{directory}/{cutoff_times_file}', parse_dates = ['time'])\n",
    "cutoff_times = cutoff_times.drop_duplicates(subset = ['msno', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "First we'll make the set of features using a single partiton so we don't have to recalculate them for each partition. This also ensures that the same exact features are made for each subset of customers. (It also is possible to load in calculated features from disk.) Again, I'm skipping the explanation for what is going on here so check out the [Featuretools documentation](https://featuretools.alteryx.com/) or some of the [online tutorials](https://www.featuretools.com/demos). \n",
    "\n",
    "### Features for One Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: customers\n",
       "  DataFrames:\n",
       "    members [Rows: 6817, Columns: 6]\n",
       "    transactions [Rows: 21992, Columns: 13]\n",
       "    logs [Rows: 418190, Columns: 13]\n",
       "  Relationships:\n",
       "    transactions.msno -> members.msno\n",
       "    logs.msno -> members.msno"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty entityset\n",
    "es = ft.EntitySet(id = 'customers')\n",
    "\n",
    "# Add the members parent table\n",
    "es.add_dataframe(dataframe_name='members', dataframe=members,\n",
    "                 index = 'msno', time_index = 'registration_init_time', \n",
    "                 logical_types = {'city': 'Categorical', 'bd': 'Categorical',\n",
    "                                  'registered_via': 'Categorical'})\n",
    "# Create new features in transactions\n",
    "trans['price_difference'] = trans['plan_list_price'] - trans['actual_amount_paid']\n",
    "trans['planned_daily_price'] = trans['plan_list_price'] / trans['payment_plan_days']\n",
    "trans['daily_price'] = trans['actual_amount_paid'] / trans['payment_plan_days']\n",
    "\n",
    "# Add the transactions child table\n",
    "es.add_dataframe(dataframe_name='transactions', dataframe=trans,\n",
    "                 index = 'transactions_index', make_index = True,\n",
    "                 time_index = 'transaction_date', \n",
    "                 logical_types = {'payment_method_id': 'Categorical', \n",
    "                                  'is_auto_renew': 'Boolean', 'is_cancel': 'Boolean'})\n",
    "\n",
    "# Add transactions interesting values\n",
    "es.add_interesting_values(dataframe_name='transactions',\n",
    "                          values={'is_cancel': [False, True],\n",
    "                                  'is_auto_renew': [False, True]})\n",
    "\n",
    "# Create new features in logs\n",
    "logs['total'] = logs[['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum(axis = 1)\n",
    "logs['percent_100'] = logs['num_100'] / logs['total']\n",
    "logs['percent_unique'] = logs['num_unq'] / logs['total']\n",
    "\n",
    "# Add the logs child table\n",
    "es.add_dataframe(dataframe_name='logs', dataframe=logs,\n",
    "                 index = 'logs_index', make_index = True,\n",
    "                 time_index = 'date')\n",
    "\n",
    "# Add the relationships\n",
    "r_member_transactions = ft.Relationship(es, 'members', 'msno', 'transactions', 'msno')\n",
    "r_member_logs = ft.Relationship(es, 'members', 'msno', 'logs', 'msno')\n",
    "es.add_relationships([r_member_transactions, r_member_logs])\n",
    "\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Primitives\n",
    "\n",
    "Below is a custom primitive we wrote (see the `Feature Engineering` notebook) for this dataset. It calculates the total amount of a quantity in the previous month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_previous_month(numeric, datetime, time):\n",
    "    \"\"\"Return total of `numeric` column in the month prior to `time`.\"\"\"\n",
    "    df = pd.DataFrame({'value': numeric, 'date': datetime})\n",
    "    previous_month = time.month - 1\n",
    "    year = time.year\n",
    "   \n",
    "    # Handle January\n",
    "    if previous_month == 0:\n",
    "        previous_month = 12\n",
    "        year = time.year - 1\n",
    "        \n",
    "    # Filter data and sum up total\n",
    "    df = df[(df['date'].dt.month == previous_month) & (df['date'].dt.year == year)]\n",
    "    total = df['value'].sum()\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featuretools.primitives import make_agg_primitive\n",
    "from woodwork.column_schema import ColumnSchema\n",
    "from woodwork.logical_types import Datetime\n",
    "\n",
    "numeric = ColumnSchema(semantic_tags={'numeric'})\n",
    "datetime = ColumnSchema(logical_type=Datetime)\n",
    "\n",
    "# Takes in a number and outputs a number\n",
    "total_previous = make_agg_primitive(total_previous_month, input_types = [numeric, datetime],\n",
    "                                    return_type = numeric, \n",
    "                                    uses_calc_time = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Deep Feature Synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we create the features, we use `ft.dfs` passing in the selected primitives, the target dataframe, the critical `cutoff_time`, the depth of the feature to stack, and several other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify aggregation primitives\n",
    "agg_primitives = ['sum', 'time_since_last', 'avg_time_between', 'num_unique', 'min', 'last', \n",
    "                  'percent_true', 'max', 'count']\n",
    "\n",
    "# Specify transformation primitives\n",
    "trans_primitives = ['is_weekend', 'cum_sum', 'day', 'month', 'time_since_previous']\n",
    "\n",
    "# Specify where primitives\n",
    "where_primitives = ['sum', 'mean', 'percent_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 316 features\n",
      "Elapsed: 24:05 | Progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████\n"
     ]
    }
   ],
   "source": [
    "# Run deep feature synthesis\n",
    "feature_matrix, feature_defs = ft.dfs(entityset=es, target_dataframe_name='members', \n",
    "                                      cutoff_time = cutoff_times, \n",
    "                                      agg_primitives = agg_primitives,\n",
    "                                      trans_primitives = trans_primitives,\n",
    "                                      where_primitives = where_primitives,\n",
    "                                      max_depth = 2, features_only = False,\n",
    "                                      chunk_size = 100, n_jobs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features definitions can be saved on disk. Every time we want to make the same exact features, we can just pass in these into the `ft.calculate_feature_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save_features(feature_defs, f'{CWD}/data/features.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 316 features.\n"
     ]
    }
   ],
   "source": [
    "feature_defs = ft.load_features(f'{CWD}/data/features.txt')\n",
    "print(f'There are {len(feature_defs)} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition to Feature Matrix Function\n",
    "\n",
    "The main function of this notebook is used to make features from a single partition. \n",
    "\n",
    "This function, `partition_to_feature_matrix`, does the following:\n",
    "\n",
    "1. Takes in the name of a partition \n",
    "2. Reads the data from the partition directory\n",
    "3. Creates an entityset from the data\n",
    "4. Computes the feature matrix for the partition\n",
    "5. Saves the feature matrix to the partition directory\n",
    "\n",
    "Note, for time and disk space limitation reason, we will only run this on the first 20 partitions, instead of the full 1000 partitions that were created in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARTITIONS = 20\n",
    "BASE_DIR = f'{CWD}/data/partitions/'\n",
    "    \n",
    "def partition_to_feature_matrix(partition, feature_defs = feature_defs, \n",
    "                                cutoff_time_name = 'MS-31_labels.csv', write = True):\n",
    "    \"\"\"Take in a partition number, create a feature matrix, and save to disk\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "        partition (int): number of partition\n",
    "        feature_defs (list of ft features): features to make for the partition\n",
    "        cutoff_time_name (str): name of cutoff time file\n",
    "        write: (boolean): whether to write the data to disk. Defaults to True\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        None: saves the feature matrix to disk\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    partition_dir = BASE_DIR + 'p' + str(partition)\n",
    "    \n",
    "    # Read in the data files\n",
    "    members = pd.read_csv(f'{partition_dir}/members.csv', \n",
    "                      parse_dates=['registration_init_time'], \n",
    "                      infer_datetime_format = True, \n",
    "                      dtype = {'gender': 'category'})\n",
    "\n",
    "    trans = pd.read_csv(f'{partition_dir}/transactions.csv',\n",
    "                       parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                        infer_datetime_format = True)\n",
    "    logs = pd.read_csv(f'{partition_dir}/logs.csv', parse_dates = ['date'])\n",
    "    \n",
    "    # Make sure to drop duplicates\n",
    "    cutoff_times = pd.read_csv(f'{partition_dir}/{cutoff_time_name}', parse_dates = ['time'])\n",
    "    cutoff_times = cutoff_times.drop_duplicates(subset = ['msno', 'time'])\n",
    "    \n",
    "    # Needed for saving\n",
    "    cutoff_spec = cutoff_time_name.split('_')[0]\n",
    "    \n",
    "    # Create empty entityset\n",
    "    es = ft.EntitySet(id = 'customers')\n",
    "\n",
    "    # Add the members parent table\n",
    "    es.add_dataframe(dataframe_name='members', dataframe=members,\n",
    "                     index = 'msno', time_index = 'registration_init_time', \n",
    "                     logical_types = {'city': 'Categorical',\n",
    "                                      'registered_via': 'Categorical'})\n",
    "    # Create new features in transactions\n",
    "    trans['price_difference'] = trans['plan_list_price'] - trans['actual_amount_paid']\n",
    "    trans['planned_daily_price'] = trans['plan_list_price'] / trans['payment_plan_days']\n",
    "    trans['daily_price'] = trans['actual_amount_paid'] / trans['payment_plan_days']\n",
    "\n",
    "    # Add the transactions child table\n",
    "    es.add_dataframe(dataframe_name='transactions', dataframe=trans,\n",
    "                     index = 'transactions_index', make_index = True,\n",
    "                     time_index = 'transaction_date', \n",
    "                     logical_types = {'payment_method_id': 'Categorical', \n",
    "                                      'is_auto_renew': 'Boolean', 'is_cancel': 'Boolean'})\n",
    "\n",
    "    # Add transactions interesting values\n",
    "    es.add_interesting_values(dataframe_name='transactions',\n",
    "                              values={'is_cancel': [False, True],\n",
    "                                      'is_auto_renew': [False, True]})\n",
    "    \n",
    "    # Create new features in logs\n",
    "    logs['total'] = logs[['num_25', 'num_50', 'num_75', 'num_985', 'num_100']].sum(axis = 1)\n",
    "    logs['percent_100'] = logs['num_100'] / logs['total']\n",
    "    logs['percent_unique'] = logs['num_unq'] / logs['total']\n",
    "    logs['seconds_per_song'] = logs['total_secs'] / logs['total'] \n",
    "    \n",
    "    # Add the logs child table\n",
    "    es.add_dataframe(dataframe_name='logs', dataframe=logs,\n",
    "                     index = 'logs_index', make_index = True,\n",
    "                     time_index = 'date')\n",
    "\n",
    "    # Add the relationships\n",
    "    r_member_transactions = ft.Relationship(es, 'members', 'msno', 'transactions', 'msno')\n",
    "    r_member_logs = ft.Relationship(es, 'members', 'msno', 'logs', 'msno')\n",
    "    es.add_relationships([r_member_transactions, r_member_logs])\n",
    "    \n",
    "    # Calculate the feature matrix using pre-calculated features\n",
    "    feature_matrix = ft.calculate_feature_matrix(entityset=es, features=feature_defs, \n",
    "                                                 cutoff_time=cutoff_times, cutoff_time_in_index = True,\n",
    "                                                 chunk_size = 1000)\n",
    "\n",
    "    if write:\n",
    "        bytes_to_write = feature_matrix.to_csv(None).encode()\n",
    "\n",
    "        with open(f'{partition_dir}/{cutoff_spec}_feature_matrix.csv', 'wb') as f:\n",
    "            f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Function\n",
    "\n",
    "Let's give the function a test with 2 different partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "partition_to_feature_matrix(10, feature_defs, 'MS-31_labels.csv', write=True)\n",
    "end = timer()\n",
    "print(f'{round(end - start)} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>time</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>AVG_TIME_BETWEEN(transactions.transaction_date)</th>\n",
       "      <th>COUNT(transactions)</th>\n",
       "      <th>LAST(transactions.actual_amount_paid)</th>\n",
       "      <th>LAST(transactions.daily_price)</th>\n",
       "      <th>...</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(logs.date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>label</th>\n",
       "      <th>days_to_churn</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2678400.0</td>\n",
       "      <td>2</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2548800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2548800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>female</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2620800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.806452</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno        time  city    bd  \\\n",
       "0  +2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=  2015-01-01   5.0  57.0   \n",
       "1  +2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=  2015-02-01   5.0  57.0   \n",
       "2  +2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=  2015-03-01   5.0  57.0   \n",
       "3  +2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=  2015-04-01   5.0  57.0   \n",
       "4  +2DC4F7/bSQhhmmAW/fys80YqMcQgTvZpvpntkPsnX0=  2015-05-01   5.0  57.0   \n",
       "\n",
       "   gender  registered_via  AVG_TIME_BETWEEN(transactions.transaction_date)  \\\n",
       "0  female             9.0                                              NaN   \n",
       "1  female             9.0                                        2678400.0   \n",
       "2  female             9.0                                        2548800.0   \n",
       "3  female             9.0                                        2548800.0   \n",
       "4  female             9.0                                        2620800.0   \n",
       "\n",
       "   COUNT(transactions)  LAST(transactions.actual_amount_paid)  \\\n",
       "0                    1                                  149.0   \n",
       "1                    2                                  149.0   \n",
       "2                    3                                  149.0   \n",
       "3                    3                                  149.0   \n",
       "4                    4                                  149.0   \n",
       "\n",
       "   LAST(transactions.daily_price)  ...  \\\n",
       "0                        4.806452  ...   \n",
       "1                        4.806452  ...   \n",
       "2                        4.806452  ...   \n",
       "3                        4.806452  ...   \n",
       "4                        4.806452  ...   \n",
       "\n",
       "  DAY(LAST(transactions.transaction_date)) IS_WEEKEND(LAST(logs.date))  \\\n",
       "0                                      1.0                       False   \n",
       "1                                      1.0                        True   \n",
       "2                                      1.0                       False   \n",
       "3                                      1.0                       False   \n",
       "4                                      2.0                       False   \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.membership_expire_date))  \\\n",
       "0                                               True       \n",
       "1                                               True       \n",
       "2                                              False       \n",
       "3                                              False       \n",
       "4                                              False       \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.transaction_date))  MONTH(LAST(logs.date))  \\\n",
       "0                                            False                     NaN   \n",
       "1                                             True                     2.0   \n",
       "2                                             True                     2.0   \n",
       "3                                             True                     4.0   \n",
       "4                                            False                     5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.membership_expire_date))  \\\n",
       "0                                               2.0   \n",
       "1                                               3.0   \n",
       "2                                               4.0   \n",
       "3                                               4.0   \n",
       "4                                               5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.transaction_date))  label  days_to_churn  \\\n",
       "0                                         1.0    0.0            NaN   \n",
       "1                                         2.0    0.0            NaN   \n",
       "2                                         3.0    0.0            NaN   \n",
       "3                                         3.0    0.0            NaN   \n",
       "4                                         4.0    0.0            NaN   \n",
       "\n",
       "   churn_date  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 321 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv(f'{BASE_DIR}/p10/MS-31_feature_matrix.csv', low_memory=False)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "partition_to_feature_matrix(19, feature_defs, 'MS-31_labels.csv', write=True)\n",
    "end = timer()\n",
    "print(f'{round(end - start)} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>time</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>AVG_TIME_BETWEEN(transactions.transaction_date)</th>\n",
       "      <th>COUNT(transactions)</th>\n",
       "      <th>LAST(transactions.actual_amount_paid)</th>\n",
       "      <th>LAST(transactions.daily_price)</th>\n",
       "      <th>...</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(logs.date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>label</th>\n",
       "      <th>days_to_churn</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2419200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2548800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2563200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>149.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno        time  city   bd gender  \\\n",
       "0  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-01-01   5.0  0.0    NaN   \n",
       "1  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-02-01   5.0  0.0    NaN   \n",
       "2  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-03-01   5.0  0.0    NaN   \n",
       "3  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-04-01   5.0  0.0    NaN   \n",
       "4  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-05-01   5.0  0.0    NaN   \n",
       "\n",
       "   registered_via  AVG_TIME_BETWEEN(transactions.transaction_date)  \\\n",
       "0             7.0                                              NaN   \n",
       "1             7.0                                              NaN   \n",
       "2             7.0                                        2419200.0   \n",
       "3             7.0                                        2548800.0   \n",
       "4             7.0                                        2563200.0   \n",
       "\n",
       "   COUNT(transactions)  LAST(transactions.actual_amount_paid)  \\\n",
       "0                    0                                    NaN   \n",
       "1                    1                                  149.0   \n",
       "2                    2                                  149.0   \n",
       "3                    3                                  149.0   \n",
       "4                    4                                  149.0   \n",
       "\n",
       "   LAST(transactions.daily_price)  ...  \\\n",
       "0                             NaN  ...   \n",
       "1                        4.966667  ...   \n",
       "2                        4.966667  ...   \n",
       "3                        4.966667  ...   \n",
       "4                             inf  ...   \n",
       "\n",
       "  DAY(LAST(transactions.transaction_date)) IS_WEEKEND(LAST(logs.date))  \\\n",
       "0                                      NaN                       False   \n",
       "1                                     31.0                        True   \n",
       "2                                     28.0                        True   \n",
       "3                                     31.0                       False   \n",
       "4                                     30.0                       False   \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.membership_expire_date))  \\\n",
       "0                                              False       \n",
       "1                                               True       \n",
       "2                                              False       \n",
       "3                                              False       \n",
       "4                                               True       \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.transaction_date))  MONTH(LAST(logs.date))  \\\n",
       "0                                            False                     1.0   \n",
       "1                                             True                     2.0   \n",
       "2                                             True                     3.0   \n",
       "3                                            False                     4.0   \n",
       "4                                            False                     5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.membership_expire_date))  \\\n",
       "0                                               NaN   \n",
       "1                                               2.0   \n",
       "2                                               3.0   \n",
       "3                                               4.0   \n",
       "4                                               5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.transaction_date))  label  days_to_churn  \\\n",
       "0                                         NaN    0.0          304.0   \n",
       "1                                         1.0    0.0          273.0   \n",
       "2                                         2.0    0.0          245.0   \n",
       "3                                         3.0    0.0          214.0   \n",
       "4                                         4.0    0.0          184.0   \n",
       "\n",
       "   churn_date  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 321 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv(f'{BASE_DIR}/p19/MS-31_feature_matrix.csv', low_memory = False)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with Spark\n",
    "\n",
    "The next cell parallelizes all the feature engineering calculations using Spark. We want to `map` the partitions to the function and we let Spark divide the work between the executors, each of which is one core on one machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create list of partitions\n",
    "partitions = list(range(N_PARTITIONS))\n",
    "\n",
    "# Create Spark context - update based on your config\n",
    "sc = pyspark.SparkContext(master = 'spark://AMB-R09BLVCJ:7077',\n",
    "                          appName = 'featuretools', conf = conf)\n",
    "\n",
    "# Parallelize feature engineering\n",
    "r = sc.parallelize(partitions, numSlices=N_PARTITIONS).\\\n",
    "    map(lambda x: partition_to_feature_matrix(x, feature_defs,\n",
    "                                              'MS-31_labels.csv')).collect()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the run is going on, we can look at the status of the cluster at localhost:8080 and the state of the particular job at localhost:4040. \n",
    "\n",
    "__Here is the overall state of the cluster.__\n",
    "\n",
    "![](../images/spark_cluster2.png)\n",
    "\n",
    "__Here is information about the submitted job.__\n",
    "\n",
    "![](../images/spark_job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the Data\n",
    "\n",
    "From here, we could read in all the partitioned feature matrices and build a single feature matrix, or if we have a model that supports [incremental (also known as on-line) learning](https://en.wikipedia.org/wiki/Incremental_learning), we can train it with one partition at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>time</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>AVG_TIME_BETWEEN(transactions.transaction_date)</th>\n",
       "      <th>COUNT(transactions)</th>\n",
       "      <th>LAST(transactions.actual_amount_paid)</th>\n",
       "      <th>LAST(transactions.daily_price)</th>\n",
       "      <th>...</th>\n",
       "      <th>DAY(LAST(transactions.transaction_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(logs.date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>IS_WEEKEND(LAST(transactions.transaction_date))</th>\n",
       "      <th>MONTH(LAST(logs.date))</th>\n",
       "      <th>MONTH(LAST(transactions.membership_expire_date))</th>\n",
       "      <th>MONTH(LAST(transactions.transaction_date))</th>\n",
       "      <th>label</th>\n",
       "      <th>days_to_churn</th>\n",
       "      <th>churn_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2419200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2548800.0</td>\n",
       "      <td>3</td>\n",
       "      <td>149.0</td>\n",
       "      <td>4.966667</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2563200.0</td>\n",
       "      <td>4</td>\n",
       "      <td>149.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           msno        time  city   bd gender  \\\n",
       "0  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-01-01   5.0  0.0    NaN   \n",
       "1  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-02-01   5.0  0.0    NaN   \n",
       "2  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-03-01   5.0  0.0    NaN   \n",
       "3  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-04-01   5.0  0.0    NaN   \n",
       "4  ++sZWXQ1v6+vf3GHez2B+CmMxTBSy8lEONF6d5E3bcI=  2015-05-01   5.0  0.0    NaN   \n",
       "\n",
       "   registered_via  AVG_TIME_BETWEEN(transactions.transaction_date)  \\\n",
       "0             7.0                                              NaN   \n",
       "1             7.0                                              NaN   \n",
       "2             7.0                                        2419200.0   \n",
       "3             7.0                                        2548800.0   \n",
       "4             7.0                                        2563200.0   \n",
       "\n",
       "   COUNT(transactions)  LAST(transactions.actual_amount_paid)  \\\n",
       "0                    0                                    NaN   \n",
       "1                    1                                  149.0   \n",
       "2                    2                                  149.0   \n",
       "3                    3                                  149.0   \n",
       "4                    4                                  149.0   \n",
       "\n",
       "   LAST(transactions.daily_price)  ...  \\\n",
       "0                             NaN  ...   \n",
       "1                        4.966667  ...   \n",
       "2                        4.966667  ...   \n",
       "3                        4.966667  ...   \n",
       "4                             inf  ...   \n",
       "\n",
       "  DAY(LAST(transactions.transaction_date)) IS_WEEKEND(LAST(logs.date))  \\\n",
       "0                                      NaN                       False   \n",
       "1                                     31.0                        True   \n",
       "2                                     28.0                        True   \n",
       "3                                     31.0                       False   \n",
       "4                                     30.0                       False   \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.membership_expire_date))  \\\n",
       "0                                              False       \n",
       "1                                               True       \n",
       "2                                              False       \n",
       "3                                              False       \n",
       "4                                               True       \n",
       "\n",
       "   IS_WEEKEND(LAST(transactions.transaction_date))  MONTH(LAST(logs.date))  \\\n",
       "0                                            False                     1.0   \n",
       "1                                             True                     2.0   \n",
       "2                                             True                     3.0   \n",
       "3                                            False                     4.0   \n",
       "4                                            False                     5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.membership_expire_date))  \\\n",
       "0                                               NaN   \n",
       "1                                               2.0   \n",
       "2                                               3.0   \n",
       "3                                               4.0   \n",
       "4                                               5.0   \n",
       "\n",
       "   MONTH(LAST(transactions.transaction_date))  label  days_to_churn  \\\n",
       "0                                         NaN    0.0          304.0   \n",
       "1                                         1.0    0.0          273.0   \n",
       "2                                         2.0    0.0          245.0   \n",
       "3                                         3.0    0.0          214.0   \n",
       "4                                         4.0    0.0          184.0   \n",
       "\n",
       "   churn_date  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "\n",
       "[5 rows x 321 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = pd.read_csv(f'{BASE_DIR}/p19/MS-31_feature_matrix.csv', low_memory = False)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we saw how to distribute feature engineering in Featuretools using the Spark framework. This big-data processing technology lets us use multiple computers to parallelize calculations, resulting in efficient data science workflows even on large datasets. \n",
    "\n",
    "The basic approach is:\n",
    "\n",
    "1. Divide data into independent partitions\n",
    "2. Run each subset in parallel with a different worker\n",
    "3. Join results together if necessary \n",
    "\n",
    "The nice part about using frameworks such as Dask and Spark with PySpark is we don't have to change the underlying Featuretools code. We write our code in native Python, change the backend running the calculations, and distribute the calculations across a cluster of machines. Using this approach, we'll be able to scale to any size datasets and take on even more exciting data science and machine learning problems. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "The final step of the machine learning pipeline is to build a model to make predictions for these features. This is implemented in the `Modeling` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
