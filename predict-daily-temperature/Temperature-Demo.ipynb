{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import feature_importances, remove_nans, split_with_gap\n",
    "import featuretools as ft\n",
    "from featuretools.primitives import RollingMean, NumericLag\n",
    "import woodwork as ww\n",
    "from evalml import AutoMLSearch\n",
    "from evalml.model_understanding import graph_prediction_vs_actual_over_time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2fa43",
   "metadata": {},
   "source": [
    "In this demo, we'll work to predict future daily average temperatures using historical temperature data. This is a time series machine learning problem, which requires special considerations during preprocessing, feature engineering, and model building.\n",
    "\n",
    "To highlight the proess through which we can solve a time series problem, we'll build three models.\n",
    "\n",
    "First, we'll build a baseline model; this will highlight the unique constraints for data-splitting and allow us to understand the problem definition. Then, we'll explore time series feature engineering by generating features with Featuretools. Finally, we'll use EvalML's time series modeling to greatly simplify the process through which these models are built.\n",
    "\n",
    "## Understanding Time Series Problems\n",
    "\n",
    "Time series forecasting is different from other machine learning problems in that there is an inherent temporal ordering to the data. The ordering comes from a time index column, so at a specific point in time, we may have knowlege about earlier observations but not later ones. If the data is unordered, it’d be hard to see any overall trend or seasonality, but when sorted by date, any relationships that exist in the data can be seen and used when making predictions (winter is cold; summer is hot!). Notice how this is different from non-time series data, which can be presented in any order without having an impact on the resulting predictions.\n",
    "\n",
    "Other demos in this repository explore this concept some. predict-remaining-useful-life, predict olympic medals, predict-appointment-noshow all have time indices that play a large roll in splitting the data for feature engineering. We can set a `cutoff_time` for feature engineering after which we do not have access to data. This is very useful for datasets that have multiple tables with relationships; we can build features from aggregations across tables. \n",
    "\n",
    "In this demo, we'll only have one table worth of data, but its temporal ordering means that we have access to a column's own historical data for feature engineering. When trying to determine tomorrow's temperature, knowing today's temperature may be the most predictive piece of information we can get. Realistically, we may not have data from so recent a time, but the concept stands; utilizing the most recent information we have is the bread and butter of time series modeling.\n",
    "\n",
    "In a time series problem, our task is to predict the future values of our target variable. If we engineer the right features, we can use normal regression models; but we need to account for the temporal ordering of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7018874e",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "We’ll demonstrate how to build a time series model using the DailyDelhiClimateTrain dataset, which contains a `meantemp` target variable and a `date` time index. There are other columns, but for the purposes of simplicity, we'll only work with the target and time index columns. To include the others would bring this demo into the sphere of multivariate time series modeling, which brings its own host of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529406e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_name = \"DailyDelhiClimateTrain\"\n",
    "df = pd.read_csv(f\"data/{file_name}.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040b51c",
   "metadata": {},
   "source": [
    "Now, we'll do a quick sanity check that the data has some temporal pattern that we can exploit for modeling purposes.\n",
    "\n",
    "First, we'll use a Woodwork method to check whether there is any column with a uniform sampling frequency. This is important, because it means that there is a constant amount of time between observations. A dataset that does not have a uniform sampling frequency can still be used for time series modeling, but the existence of that frequency is a good indicator that this dataset is ripe for time series modeling. For columns that have multiple datetime columns, checking for a frequency is also a good indicator for which could be the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ww.init()\n",
    "df.ww.infer_temporal_frequencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d559a",
   "metadata": {},
   "source": [
    "Indeed, one of the columns, `date`, has a daily frequency; we'll move forward with it as our time index.\n",
    "\n",
    "Now, we’ll graph the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c5e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = df['meantemp']\n",
    "ts.index = df['date']\n",
    "ts.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6942a",
   "metadata": {},
   "source": [
    "We can see a strong seasonality, which makes sense for temperature! In many places, the time of the year is indicative of what the weather will look like. Now, we'll build a baseline model that uses the most recently available data. \n",
    "\n",
    "But how do we define what data we have access to when? In many scenarios, this might be be determined by quickly we can get access to recent observations. Since we're building a model using training and test data that we'll have access to right now, we'll need to set some parameters arbitrarily. But these parameters will let us define the problem more formally. We'll stick with these definitions problem configuration throughout the rest of the demo. \n",
    "\n",
    "## Problem Configuration\n",
    "Here are a few concepts that give us our official problem configuration:\n",
    "\n",
    "**forecast_horizon**: The number of time periods we are trying to forecast. In this example, we’re interested in predicting the mean temperature for the next 5 days, so the value is 5.\n",
    "\n",
    "**gap**: The number of time periods between the end of the training set and the start of the test set. We’re going to make predictions using data from three days prior to each observation.\n",
    "\n",
    "**max_delay**: The maximum number of rows to look in the past from the current row in order to compute features. Here, we’ll use a max delay of 20.\n",
    "\n",
    "**time_index**: The column of the training dataset that contains the date corresponding to each observation. Here, it's the `date` column.\n",
    "\n",
    "Our problem can then be described as trying to predict the mean temperature over the next five days using temperature data from 20 days prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca1fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only columns we'll want to use for modeling - makes this a univariate problem\n",
    "time_index = \"date\"\n",
    "target_col = 'meantemp'\n",
    "\n",
    "# parameters as evalml uses them \n",
    "gap = 3\n",
    "max_delay = 20\n",
    "forecast_horizon = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a8685",
   "metadata": {},
   "source": [
    "Since we do not want to complicate the solition by performing multivariate time series modeling, we'll only use the time index column and target column for the rest of this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_df = df[[time_index, target_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bad89",
   "metadata": {},
   "source": [
    "Additionally, we'll want to have our data split up into training and testing data. We'll use the same split for both our baseline and Featuretools run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cc42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(univariate_df.shape[0]*.8)\n",
    "\n",
    "# leave gap observations between training and test datasets\n",
    "training_data = univariate_df[:split_point]\n",
    "test_data = univariate_df[(split_point + gap):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6ed90f",
   "metadata": {},
   "source": [
    "# Baseline Run\n",
    "\n",
    "Our baseline run will only include one feature that is shifted to the first known value for each observation. When splitting data, we'll need to be careful to not have the test dataset's lag feature use values that are technically before the test set begins or inside of the training set. \n",
    "\n",
    "First, let's split the data, leaving a `gap` number of observations between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2737ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a delayed target feature to both traning and test data\n",
    "target_lag_training = training_data[target_col].shift(forecast_horizon + gap + 1)\n",
    "target_lag_training.name = 'target_lag'\n",
    "baseline_training = pd.concat([training_data, target_lag_training], axis=1)\n",
    "\n",
    "target_lag_test = test_data[target_col].shift(forecast_horizon + gap + 1)\n",
    "target_lag_test.name = 'target_lag'\n",
    "baseline_test = pd.concat([test_data, target_lag_test], axis=1)\n",
    "\n",
    "# Get rid of the time index column for modeling\n",
    "baseline_training.drop(time_index, axis=1, inplace=True)\n",
    "baseline_test.drop(time_index, axis=1, inplace=True)\n",
    "\n",
    "# The lag feature introduces nans, so we remove those rows and pull out the target\n",
    "X_train, y_train = remove_nans(baseline_training, target_col)\n",
    "X_test, y_test = remove_nans(baseline_test, target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=100)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "preds = reg.predict(X_test)\n",
    "scores = median_absolute_error(preds, y_test)\n",
    "print('Median Abs Error: {:.2f}'.format(scores))\n",
    "\n",
    "high_imp_feats = feature_importances(X_train, reg, feats=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d9a32",
   "metadata": {},
   "source": [
    "We can build more features, some of which may be similar to the `lag` feature we used in the baseline model, but if we look back at the graph with the rolling mean, we remember that rolling mean was a really good indicator for the mean temp. So we'll want a way of including that as a feature without exposing our target. This is where Featuretools' time series primitives comes into play. We'll also add some more standard datetime primitives that might have predictive power; for example, the month of the year is a very good indicator of what the teamperature should be.\n",
    "\n",
    "# Feature Engineering Run \n",
    "\n",
    "First we'll split our data in exactly the same way that we did for the baseline run\n",
    "### Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b32e54",
   "metadata": {},
   "source": [
    "### Feature Engineering with Featuretools\n",
    "\n",
    "Now, we engineer some time series-specific features. We'll recreate _______\n",
    "EXPLAIN WHY WE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as featuretools will use them\n",
    "rolling_gap = forecast_horizon + gap\n",
    "rolling_window_length = int(.25*max_delay) + 1 # a quarter is a heuristic here \n",
    "rolling_min_periods = int(.25*max_delay) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995348c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_es = ft.EntitySet()\n",
    "training_es.add_dataframe(training_data.copy(), \n",
    "                 dataframe_name='temperatures', \n",
    "                 index='id', \n",
    "                 make_index=True, \n",
    "                 time_index=time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb6c9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_es = ft.EntitySet()\n",
    "test_es.add_dataframe(test_data.copy(), \n",
    "                 dataframe_name='temperatures', \n",
    "                 index='id', \n",
    "                 make_index=True, \n",
    "                 time_index=time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496c781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datetime_featureizer = [ 'Month', 'Hour', \"Year\"]\n",
    "# how is the statistically significant lags from evalml that makes up the nubmer of lags determined? \n",
    "# max delay - dets the number of features (up to - can pick any one/number )\n",
    "lagging_featureizer = [NumericLag(periods=t + forecast_horizon + gap) for t in range(forecast_horizon + gap + 1)]\n",
    "\n",
    "\n",
    "train_fm, features = ft.dfs(entityset=training_es, \n",
    "               target_dataframe_name='temperatures', \n",
    "               max_depth=1,\n",
    "               trans_primitives = datetime_featureizer + lagging_featureizer +[ \n",
    "                                   RollingMean(rolling_window_length, \n",
    "                                               gap=rolling_gap,\n",
    "                                              min_periods=rolling_min_periods)]\n",
    "              )\n",
    "\n",
    "X_train, y_train = remove_nans(train_fm, target_col)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c8b0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_fm = ft.calculate_feature_matrix(features, test_es)\n",
    "\n",
    "X_test, y_test = remove_nans(test_fm, target_col)\n",
    "\n",
    "\n",
    "test_fm.ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=100)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "preds = reg.predict(X_test)\n",
    "scores = median_absolute_error(preds, y_test)\n",
    "print('Median Abs Error: {:.2f}'.format(scores))\n",
    "\n",
    "high_imp_feats = feature_importances(X_train, reg, feats=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16057c8",
   "metadata": {},
   "source": [
    "Looking at the feature importances above, we see that the rolling mean was, indeed, very predictive along with the Month feature. \n",
    "\n",
    "## Use Time Series Regression Problem From EvalML\n",
    "We will now build a model that is very similar to the one we just built with the help of Featuretools. EvalML's time series regression problem type does the same feature engineering that we just did under the hood. That, along with some other optimizations and the fact that we run multiple pipelines shows the power of EvalML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evalml\n",
    "\n",
    "univariate_df = df[[time_index, target_col]]\n",
    "\n",
    "X = univariate_df\n",
    "y = univariate_df.pop(target_col)\n",
    "\n",
    "X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y,\n",
    "                                                                   problem_type='time series regression',\n",
    "                                                                   test_size=.2,\n",
    "                                                                  problem_configuration={\"gap\": gap, \"max_delay\": max_delay,\n",
    "                                             \"forecast_horizon\": forecast_horizon, \"time_index\": time_index},)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e17b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evalml import AutoMLSearch\n",
    "\n",
    "automl = AutoMLSearch(X_train, y_train, problem_type=\"time series regression\",\n",
    "                      max_batches=1,\n",
    "                      problem_configuration={\"gap\": gap, \"max_delay\": max_delay,\n",
    "                                             \"forecast_horizon\": forecast_horizon, \"time_index\": time_index},\n",
    "                      allowed_model_families=[\"xgboost\", \"random_forest\", \"linear_model\", \"extra_trees\",\n",
    "                                              \"decision_tree\"],\n",
    "                      objective='MedianAE'\n",
    "                      )\n",
    "automl.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90185ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2d109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = automl.best_pipeline\n",
    "pipeline.feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb952824",
   "metadata": {},
   "source": [
    "Look at how similar the feature importances are! The top three are all the same most of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20751c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline_score = pipeline.score(X_test, y_test, ['MedianAE'], X_train, y_train)['MedianAE']\n",
    "best_pipeline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = automl.get_pipeline(0)\n",
    "baseline.fit(X_train, y_train)\n",
    "naive_baseline_score = baseline.score(X_test, y_test, ['MedianAE'], X_train, y_train)['MedianAE']\n",
    "naive_baseline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = graph_prediction_vs_actual_over_time(pipeline, X_test, y_test, X_train, y_train, dates=X_test['date'])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = graph_prediction_vs_actual_over_time(baseline, X_test, y_test, X_train, y_train, dates=X_test['date'])\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
