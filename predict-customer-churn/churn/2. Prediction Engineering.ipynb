{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Prediction Engineering: Labeling Historical Examples\n",
    "\n",
    "In this notebook, we will develop a method for labeling customer transactions data for a customer churn prediction problem. The objective of labeling is to create a set of historical examples of what we want to predict based on the business need: in this problem, our goal is to predict customer churn, so we want to create labeled examples of past churn from the data.\n",
    "\n",
    "The end outcome of this notebook is a set of labels each with an associated cutoff time in a table called a label times table. These labels with cutoff times can later be used in Featuretools for automated feature engineering. These features in turn will be used to train a predictive model to forecast customer churn, a common need for subscription-based business models, and one for which machine learning is well-suited.\n",
    "\n",
    "The process of prediction engineering is shown below:\n",
    "\n",
    "![](../images/prediction_engineering_process.png)\n",
    "\n",
    "## Definition of Churn: Prediction Problems\n",
    "\n",
    "The definition of churn is __a customer going without an active membership for a certain number of days.__ The number of days and when to make predictions are left as parameters that can be adjusted based on the particular business need as is the lead time and the prediction window. In this notebook, we'll make labels for two scenarios:\n",
    "\n",
    "1. Monthly churn\n",
    "    * Prediction date = first of month\n",
    "    * Number of days to churn = 31\n",
    "    * Lead time = 1 month\n",
    "    * Prediction window = 1 month\n",
    "2. Bimonthly churn\n",
    "    * Prediction date = first and fifteenth of month\n",
    "    * Number of days to churn = 14\n",
    "    * Lead time = 2 weeks\n",
    "    * Prediction window = 2 weeks\n",
    "    \n",
    "The problem parameters with details filled in for the first situation are shown below:\n",
    "\n",
    "![](../images/churn_definition.png)\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The [data (publicly available)](https://www.kaggle.com/c/kkbox-churn-prediction-challenge/data) consists of customer transactions for [KKBOX](https://www.kkbox.com), the leading music subscription streaming service in Asia.\n",
    "For each customer, we have background information (in `members`), logs of listening behavior (in `logs`), and transactions information (in `trans`). The only data we need for labeling is the _transactions information_.\n",
    "\n",
    "The transactions data consists of a number of columns, the most important of which are customer id (`msno`), the date of transaction (`transaction_date`), and the expiration date of the membership (`membership_expire_date`). Using these columns, we can find each churn for each customer and the corresponding date on which it occurred. Let's look at a few typical examples of customer transaction data to illustrate how to find a churn example. For these examples, we will use the first prediction problem.\n",
    "\n",
    "## Churn Examples\n",
    "\n",
    "__Example 1:__\n",
    "\n",
    "```\n",
    "(transaction_date, membership_expire_date, is_cancel)\n",
    "\n",
    "(2017-01-01, 2017-02-28, false)\n",
    "\n",
    "(2017-02-25, 0217-03-15, false)\n",
    "\n",
    "(2017-04-31, 3117-05-20, false)\n",
    "```\n",
    "This customer is a churn because they go without a membership for over 31 days, from 03-15 to 04-31. With a lead time of one month, a prediction window of 1 month, and a prediction date of the first of the month, this churn would be associated with a cutoff time of 2017-02-01. \n",
    "\n",
    "__Example 2:__\n",
    "```\n",
    "(transaction_date, membership_expire_date, is_cancel)\n",
    "\n",
    "(2017-01-01, 2017-02-28, false)\n",
    "\n",
    "(2017-02-25, 2017-04-03, false)\n",
    "\n",
    "(2017-03-15, 2017-03-16, true)\n",
    "\n",
    "(2017-04-01, 3117-06-31, false)\n",
    "```\n",
    "\n",
    "This customer is not a churn. Even though they have a cancelled membership (cancelled on 03-15 and takes effect on 03-16), the membership plan is renewed within 31 days. \n",
    "\n",
    "__Example 3:__\n",
    "```\n",
    "(transaction_date, membership_expire_date, is_cancel)\n",
    "\n",
    "(2017-05-31, 2017-06-31, false)\n",
    "\n",
    "(2017-07-01, 2017-08-01, false)\n",
    "\n",
    "(2017-08-01, 2017-09-01, false)\n",
    "\n",
    "(2017-10-15, 2017-11-15, false)\n",
    "```\n",
    "This customer is a churn because they go without a membership for over 31 days, from 09-01 to 10-15. The associated cutoff time of this churn in 2017-09-01. \n",
    "\n",
    "These three examples illustrate different situations that occur in the data. Depending on the prediction problem, these may or may not be churns and can be assigned to different cutoff times. \n",
    "\n",
    "# Approach\n",
    "\n",
    "Given the data above, to find each example of churn, we need to find the difference between one `membership_expire_date` and the next `transaction_date`. If this period is greater than the days selected for a churn, then this is a positive example of churn. For each churn, we can find the exact date on which it occurred by adding the number of days for a churn to the `membership_expire_date` associated with the churn. We create a set of cutoff times using the prediction date parameter and then for each positive label, determine the cutoff time for the churn. As an example, if the churn occurs on 09-15 with a lead time of 1 month and a prediction window of 1 month, then this churn gets the cutoff time 08-01. Cutoff times where the customer was active 1-2 months out (for this problem) will receive a negative label, and, cutoff times where we cannot determine whether the customer was active or was a churn, will not be labeled. \n",
    "\n",
    "We can very rapidly label customer transactions by shifting each `transaction_date` back by one and matching it to the previous `membership_expire_date`. We then find the difference in days between these two (`transaction` - `expire`) and if the difference is greater than the number of days established for churn, this is a positive label. Once we have these positive labels, associating them with a cutoff time is straightforward. \n",
    "\n",
    "If this is not clear, we'll shortly see how to do it in code which should clear things up! \n",
    "\n",
    "The general framework is implemented in two functions:\n",
    "\n",
    "1. `label_customer(customer_id, transactions, **params)`\n",
    "2. `make_label_times(transactions, **params)` \n",
    "\n",
    "The first takes a single member and returns a table of cutoff times for the member along with the associated labels. The second goes through all of the customers and applies the `customer_to_label_times` function to each one. The end outcome is a single table consisting of the label times for each customer. Since we already partitioned the data, we can run this function over multiple partitions in parallel to rapidly label all the data.\n",
    "\n",
    "## Cutoff Times\n",
    "\n",
    "A critical part of the label times table is the cutoff time associated with each label. This time at which we make a prediction are referred to as _cutoff_ times and they represent when all our data for making features for that particular label must be before. For instance, if our cutoff time is July 1, and we want to make predictions of churn during the month of August, all of our features for this label must be made with data from before July 1. Cutoff times are a critical consideration when feature engineering for time-series problems to prevent data leakage. Later when we go to perform automated feature engineering, Featuretools will automatically filter data based on the cutoff times so we don't have to worry about invalid training data.\n",
    "\n",
    "### Outcome\n",
    "\n",
    "Our overall goal is to build two functions that will generate labels for customers. We can then run this function over our partitions in parallel (our data has been partitioned in 1000 segments, each containing a random subset of customers). Once the label dataframes with cutoff times have been created, we can use them for automated feature engineering using Featuretools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:09.057824Z",
     "start_time": "2018-10-31T16:37:08.878346Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage \n",
    "\n",
    "For this demonstration, all of the data is stored and written locally. However, you could also store data in AWS S3 by simply changing `BASE_DIR` to use an S3 path and properly configuring your credentials to write to the bucket. You could also complete the work on AWS EC2 instances which makes retrieving and writing data to S3 extremely fast.\n",
    "\n",
    "The benefits of using S3 are that if we shut off our machines, we don't have to worry about losing any of the data. It also makes it easier to run computations in parallel across many machines with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTITION = '100'\n",
    "CWD = os.getcwd()\n",
    "BASE_DIR = f'{CWD}/data/partitions/'\n",
    "PARTITION_DIR = BASE_DIR + 'p' + PARTITION\n",
    "\n",
    "members = pd.read_csv(f'{PARTITION_DIR}/members.csv', \n",
    "                      parse_dates=['registration_init_time'], infer_datetime_format = True)\n",
    "trans = pd.read_csv(f'{PARTITION_DIR}/transactions.csv',\n",
    "                   parse_dates=['transaction_date', 'membership_expire_date'], infer_datetime_format = True)\n",
    "logs = pd.read_csv(f'{PARTITION_DIR}/logs.csv', parse_dates = ['date'])\n",
    "\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transactions table is all we will need to make labels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn for One Customer\n",
    "\n",
    "The function below takes in a single customer's transactions along with a number of parameters that define the prediction problem. \n",
    "\n",
    "* `prediction_date`: when we want to make predictions\n",
    "* `churn_days`: the number of days without a membership required for a churn\n",
    "* `lead_time`: how long in advance to predict churn\n",
    "* `prediction_window`: the length of time we are considering for a churn . \n",
    "\n",
    "The return from `label_customer` is a label_times dataframe for the customer which has cutoff times for the specified `prediction_date` and the label at each prediction time. Leaving the prediction time and number of days for a churn as parameters allows us to create multiple prediction problems using the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:25.306907Z",
     "start_time": "2018-10-31T16:37:25.294964Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_customer(customer_id, customer_transactions, prediction_date, churn_days, \n",
    "                   lead_time = 1, prediction_window = 1, return_trans = False):\n",
    "    \"\"\"\n",
    "    Make label times for a single customer. Returns a dataframe of labels with times, the binary label, \n",
    "    and the number of days until the next churn.\n",
    "       \n",
    "    Params\n",
    "    --------\n",
    "        customer_id (str): unique id for the customer\n",
    "        customer_transactions (dataframe): transactions dataframe for the customer\n",
    "        prediction_date (str): time at which predictions are made. Either \"MS\" for the first of the month\n",
    "                               or \"SMS\" for the first and fifteenth of each month \n",
    "        churn_days (int): integer number of days without an active membership required for a churn. A churn is\n",
    "                          defined by exceeding this number of days without an active membership.\n",
    "        lead_time (int): number of periods in advance to make predictions for. Defaults to 1 (preditions for one offset)\n",
    "        prediction_window(int): number of periods over which to consider churn. Defaults to 1.\n",
    "        return_trans (boolean): whether or not to return the transactions for analysis. Defaults to False.\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        label_times (dataframe): a table of customer id, the cutoff times at the specified frequency, the \n",
    "                                 label for each cutoff time, the number of days until the next churn for each\n",
    "                                 cutoff time, and the date on which the churn itself occurred.\n",
    "        transactions (dataframe): [optional] dataframe of customer transactions if return_trans = True. Useful\n",
    "                                  for making sure that the function performed as expected\n",
    "    \n",
    "       \"\"\"\n",
    "    \n",
    "    assert(prediction_date in ['MS', 'SMS']), \"Prediction day must be either 'MS' or 'SMS'\"\n",
    "    assert(customer_transactions['msno'].unique() == [customer_id]), \"Transactions must be for only customer\"\n",
    "    \n",
    "    # Don't modify original\n",
    "    transactions = customer_transactions.copy()\n",
    "    \n",
    "    # Make sure to sort chronalogically\n",
    "    transactions.sort_values(['transaction_date', 'membership_expire_date'], inplace = True)\n",
    "    \n",
    "    # Create next transaction date by shifting back one transaction\n",
    "    transactions['next_transaction_date'] = transactions['transaction_date'].shift(-1)\n",
    "    \n",
    "    # Find number of days between membership expiration and next transaction\n",
    "    transactions['difference_days'] = (transactions['next_transaction_date'] - \n",
    "                                       transactions['membership_expire_date']).\\\n",
    "                                       dt.total_seconds() / (3600 * 24)\n",
    "    \n",
    "    # Determine which transactions are associated with a churn\n",
    "    transactions['churn'] = transactions['difference_days'] > churn_days\n",
    "    \n",
    "    # Find date of each churn\n",
    "    transactions.loc[transactions['churn'] == True, \n",
    "                     'churn_date'] = transactions.loc[transactions['churn'] == True, \n",
    "                                                      'membership_expire_date'] + pd.Timedelta(churn_days + 1, 'd')\n",
    "    \n",
    "    # Range for cutoff times is from first to (last + 1 month) transaction\n",
    "    first_transaction = transactions['transaction_date'].min()\n",
    "    last_transaction = transactions['transaction_date'].max()\n",
    "    start_date = datetime(first_transaction.year, first_transaction.month, 1)\n",
    "    \n",
    "    # Handle December\n",
    "    if last_transaction.month == 12:\n",
    "        end_date = datetime(last_transaction.year + 1, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(last_transaction.year, last_transaction.month + 1, 1)\n",
    "    \n",
    "    # Make label times dataframe with cutoff times corresponding to prediction date\n",
    "    label_times = pd.DataFrame({'time': pd.date_range(start_date, end_date, freq = prediction_date),\n",
    "                                'msno': customer_id\n",
    "                               })\n",
    "    \n",
    "    # Use the lead time and prediction window parameters to establish the prediction window \n",
    "    # Prediction window is for each cutoff time\n",
    "    label_times['prediction_window_start'] = label_times['time'].shift(-lead_time)\n",
    "    label_times['prediction_window_end'] = label_times['time'].shift(-(lead_time + prediction_window))\n",
    "    \n",
    "    previous_churn_date = None\n",
    "\n",
    "    # Iterate through every cutoff time\n",
    "    for i, row in label_times.iterrows():\n",
    "        \n",
    "        # Default values if unknown\n",
    "        churn_date = pd.NaT\n",
    "        label = np.nan\n",
    "        # Find the window start and end\n",
    "        window_start = row['prediction_window_start']\n",
    "        window_end = row['prediction_window_end']\n",
    "        # Determine if there were any churns during the prediction window\n",
    "        churns = transactions.loc[(transactions['churn_date'] >= window_start) & \n",
    "                                  (transactions['churn_date'] < window_end), 'churn_date']\n",
    "\n",
    "        # Positive label if there was a churn during window\n",
    "        if not churns.empty:\n",
    "            label = 1\n",
    "            churn_date = churns.values[0]\n",
    "\n",
    "            # Find number of days until next churn by \n",
    "            # subsetting to cutoff times before current churn and after previous churns\n",
    "            if not previous_churn_date:\n",
    "                before_idx = label_times.loc[(label_times['time'] <= churn_date)].index\n",
    "            else:\n",
    "                before_idx = label_times.loc[(label_times['time'] <= churn_date) & \n",
    "                                             (label_times['time'] > previous_churn_date)].index\n",
    "\n",
    "            # Calculate days to next churn for cutoff times before current churn\n",
    "            label_times.loc[before_idx, 'days_to_churn'] = (churn_date - label_times.loc[before_idx, \n",
    "                                                                                         'time']).\\\n",
    "                                                            dt.total_seconds() / (3600 * 24)\n",
    "            previous_churn_date = churn_date\n",
    "        # No churns, but need to determine if an active member\n",
    "        else:\n",
    "            # Find transactions before the end of the window that were not cancelled\n",
    "            transactions_before = transactions.loc[(transactions['transaction_date'] < window_end) & \n",
    "                                                   (transactions['is_cancel'] == False)].copy()\n",
    "            # If the membership expiration date for this membership is after the window start, the custom has not churned\n",
    "            if np.any(transactions_before['membership_expire_date'] >= window_start):\n",
    "                label = 0\n",
    "\n",
    "        # Assign values\n",
    "        label_times.loc[i, 'label'] = label\n",
    "        label_times.loc[i, 'churn_date'] = churn_date\n",
    "        \n",
    "        # Handle case with no churns\n",
    "        if not np.any(label_times['label'] == 1):\n",
    "            label_times['days_to_churn'] = np.nan\n",
    "            label_times['churn_date'] = pd.NaT\n",
    "        \n",
    "    if return_trans:\n",
    "        return label_times.drop(columns = ['msno']), transactions\n",
    "    \n",
    "    return label_times[['msno', 'time', 'label', 'days_to_churn', 'churn_date']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of this function for a typical customer. We'll take the use case of making predictions on the first of each month with 31 days required for a churn, a lead time of 1 month, and a prediction window of 1 month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:26.504521Z",
     "start_time": "2018-10-31T16:37:26.264121Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID = trans.iloc[8, 0]\n",
    "customer_transactions = trans.loc[trans['msno'] == CUSTOMER_ID].copy()\n",
    "\n",
    "label_times, cust_transactions = label_customer(CUSTOMER_ID, customer_transactions, \n",
    "                                                prediction_date = 'MS', churn_days = 31, \n",
    "                                                lead_time = 1, prediction_window = 1, return_trans = True)\n",
    "label_times.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the function worked, we'll want to take a look at the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:27.355650Z",
     "start_time": "2018-10-31T16:37:27.345963Z"
    }
   },
   "outputs": [],
   "source": [
    "cust_transactions.iloc[3:10, -7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the churn occurred on 2016-03-16 as the customer went 98 days between an active membership from 2016-02-14 to 2016-05-22. The actual churn occurs 31 days from when the membership expires. The churn is only associated with one cutoff time, 2016-02-01. This corresponds to the lead time and prediction window associated with this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the function in use for the other prediction problem, making predictions on the first and fifteenth of each month with churn defined as more than 14 days without an active membership. The lead time is set to two weeks (one prediction period) and the prediction window is also set to two weeks. To change the prediction problem, all we need to do is alter the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:28.033784Z",
     "start_time": "2018-10-31T16:37:27.964454Z"
    }
   },
   "outputs": [],
   "source": [
    "CUSTOMER_ID = trans.iloc[100, 0]\n",
    "customer_transactions = trans.loc[trans['msno'] == CUSTOMER_ID].copy()\n",
    "\n",
    "label_times, cust_transactions = label_customer(CUSTOMER_ID, customer_transactions, \n",
    "                                                prediction_date = 'SMS', churn_days = 14, \n",
    "                                                lead_time = 1, prediction_window = 1, return_trans = True)\n",
    "label_times.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several times when we can't determine if the customer churned or not because of the way the problem has been set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:28.342597Z",
     "start_time": "2018-10-31T16:37:28.332297Z"
    }
   },
   "outputs": [],
   "source": [
    "cust_transactions.iloc[:10, -7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the churn on 2016-03-15, it was assigned to the `cutoff_time` of 2016-03-01 as expected with a lead time of two weeks and a prediction window of two weeks. (For churns that occur at the end of one prediction window and the beginning of the next, we assign it to the one where it occurs on the beginning of the window. This can be quickly changed by altering the logic of the function.)\n",
    "\n",
    "The function works as designed, we can pass in different parameters and rapidly make prediction problems. We also have the number of days to the churn which means we could formulate the problem as regression instead of classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn for All Customers\n",
    "\n",
    "Next, we take the function which works for one customer and apply it to all customers in a dataset. This requires a loop through the customers by grouping the customer transactions and applying `label_customer` to each customer's transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:37:29.596542Z",
     "start_time": "2018-10-31T16:37:29.592918Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_label_times(transactions, prediction_date, churn_days, \n",
    "                   lead_time = 1, prediction_window = 1,):\n",
    "    \"\"\"\n",
    "    Make labels for an entire series of transactions. \n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "        transactions (dataframe): table of customer transactions\n",
    "        prediction_date (str): time at which predictions are made. Either \"MS\" for the first of the month\n",
    "                               or \"SMS\" for the first and fifteenth of each month \n",
    "        churn_days (int): integer number of days without an active membership required for a churn. A churn is\n",
    "                          defined by exceeding this number of days without an active membership.\n",
    "        lead_time (int): number of periods in advance to make predictions for. Defaults to 1 (preditions for one offset)\n",
    "        prediction_window(int): number of periods over which to consider churn. Defaults to 1.\n",
    "    Return\n",
    "    --------\n",
    "        label_times (dataframe): a table with customer ids, cutoff times, binary label, regression label, \n",
    "                                 and date of churn. This table can then be used for feature engineering.\n",
    "    \"\"\"\n",
    "    \n",
    "    label_times = []\n",
    "    transactions = transactions.sort_values(['msno', 'transaction_date'])\n",
    "    \n",
    "    # Iterate through each customer and find labels\n",
    "    for customer_id, customer_transactions in transactions.groupby('msno'):\n",
    "        lt_cust = label_customer(customer_id, customer_transactions,\n",
    "                                                   prediction_date, churn_days, \n",
    "                                                   lead_time, prediction_window)\n",
    "        \n",
    "        label_times.append(lt_cust)\n",
    "        \n",
    "    # Concatenate into a single dataframe\n",
    "    return pd.concat(label_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at examples of using this function for both prediction problems.\n",
    "\n",
    "## First Prediction Problem\n",
    "\n",
    "The defintion of the first prediction problem is as follows:\n",
    "\n",
    "* Monthly churn\n",
    "    * Prediction date = first of month\n",
    "    * Number of days to churn = 31\n",
    "    * Lead time = 1 month\n",
    "    * Prediction window = 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:38:09.330501Z",
     "start_time": "2018-10-31T16:37:30.500502Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times = make_label_times(trans, prediction_date = 'MS', churn_days = 31,\n",
    "                               lead_time = 1, prediction_window = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:38:09.536296Z",
     "start_time": "2018-10-31T16:38:09.526992Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:38:39.290256Z",
     "start_time": "2018-10-31T16:38:39.287200Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:38:40.844874Z",
     "start_time": "2018-10-31T16:38:40.840452Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:38:43.586867Z",
     "start_time": "2018-10-31T16:38:43.309921Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "label_times['label'].value_counts().plot.bar(color = 'r');\n",
    "plt.xlabel('Label'); plt.ylabel('Count'); plt.title('Label Distribution with Monthly Predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an imbalanced classification problem. There are far more instances of customers not churning than of customers churning. This is not necessarily an issue as long as we are smart about the choices of metrics we use for modeling. \n",
    "\n",
    "\n",
    "## Second Prediction Problem\n",
    "\n",
    "To demonstrate how to quickly change the problem parameters, we can use the labeling function for a different prediction problem. The parameters are defined below:\n",
    "\n",
    "* Bimonthly churn\n",
    "    * Prediction date = first and fifteenth of month\n",
    "    * Number of days to churn = 14\n",
    "    * Lead time = 2 weeks\n",
    "    * Prediction window = 2 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:01.887037Z",
     "start_time": "2018-10-31T16:39:12.674043Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times = make_label_times(trans, prediction_date = 'SMS', churn_days = 14,\n",
    "                               lead_time = 1, prediction_window = 1)\n",
    "label_times.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:02.094679Z",
     "start_time": "2018-10-31T16:40:02.091142Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:02.420085Z",
     "start_time": "2018-10-31T16:40:02.297025Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times['label'].value_counts().plot.bar(color = 'r');\n",
    "plt.xlabel('Label'); plt.ylabel('Count'); plt.title('Label Distribution with Bimonthly Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:02.622506Z",
     "start_time": "2018-10-31T16:40:02.618016Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times['label'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few missing labels, which occur when there is no next transaction for the customer (we don't know if the last entry for the customer is a churn or not). We won't be able to use these examples when training a model although we can make predictions for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing Labeling\n",
    "\n",
    "Now that we have a function that can make a label times table out of customer transactions, we need to label all of the customer transactions in our dataset. We already broke the data into 1000 partitions, so we can parallelize this operation using Spark with PySpark. The basic idea is to write a function that makes the label times for one partition, and then run this in parallel across all the partitions using either multiple cores on a single machine, or a cluster of machines. \n",
    "\n",
    "The function below takes in a partition number, reads the transactions data, creates the label times table for both prediction problems, and writes the label times back to the partition directory. We can run this function in parallel over multiple partitions at once since the customers are independent of one another. That is, the labels for one customer do not depend on the data for any other customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:02.832785Z",
     "start_time": "2018-10-31T16:40:02.826593Z"
    }
   },
   "outputs": [],
   "source": [
    "def partition_to_labels(partition_number, prediction_dates = ['MS', 'SMS'], churn_periods= [31, 14],\n",
    "                        lead_times = [1, 1], prediction_windows = [1, 1]):\n",
    "    \"\"\"Make labels for all customers in one partition\n",
    "    Either for one month or twice a month\n",
    "    \n",
    "    Params\n",
    "    --------\n",
    "        partition (int): number of partition\n",
    "        label_type (list of str): either 'MS' for monthly labels or\n",
    "                                  'SMS' for bimonthly labels\n",
    "        churn_periods(list of int): number of days with no active membership to be considered a churn\n",
    "        lead_times (list of int): lead times in number of periods\n",
    "        prediction_windows (list of int): prediction windows in number of periods\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "        None: saves the label dataframes with the appropriate name to the partition directory\n",
    "    \"\"\"\n",
    "    partition_dir = BASE_DIR + 'p' + str(partition_number)\n",
    "\n",
    "    # Read in data and filter anomalies\n",
    "    trans = pd.read_csv(f'file:///{partition_dir}/transactions.csv',\n",
    "                        parse_dates=['transaction_date', 'membership_expire_date'], \n",
    "                        infer_datetime_format = True)\n",
    "    \n",
    "    # Deal with data inconsistencies\n",
    "    rev = trans[(trans['membership_expire_date'] < trans['transaction_date']) | \n",
    "            ((trans['is_cancel'] == 0) & (trans['membership_expire_date'] == trans['transaction_date']))]\n",
    "    rev_members = rev['msno'].unique()\n",
    "    \n",
    "    # Remove data errors\n",
    "    trans = trans.loc[~trans['msno'].isin(rev_members)]\n",
    "\n",
    "    # Create both sets of lables\n",
    "    for prediction_date, churn_days, lead_time, prediction_window in zip(prediction_dates, churn_periods, lead_times, prediction_windows):\n",
    "        \n",
    "        cutoff_list = []\n",
    "            \n",
    "        # Make label times for all customers\n",
    "        cutoff_list.append(make_label_times(trans, prediction_date = prediction_date, \n",
    "                                            churn_days = churn_days, lead_time = lead_time,\n",
    "                                            prediction_window = prediction_window))\n",
    "        # Turn into a dataframe\n",
    "        cutoff_times = pd.concat(cutoff_list)\n",
    "        cutoff_times = cutoff_times.drop_duplicates(subset = ['msno', 'time'])\n",
    "        \n",
    "        # Write cutoff times to partition\n",
    "        bytes_to_write = cutoff_times.to_csv(None, index=False).encode()\n",
    "        with open(f'{partition_dir}/{prediction_date}-{churn_days}_labels.csv', 'wb') as f:\n",
    "            f.write(bytes_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:40.844134Z",
     "start_time": "2018-10-31T16:40:03.035551Z"
    }
   },
   "outputs": [],
   "source": [
    "partition_to_labels(1, prediction_dates = ['MS'], churn_periods = [31], \n",
    "                    lead_times = [1], prediction_windows = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:40:41.136537Z",
     "start_time": "2018-10-31T16:40:41.035112Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times = pd.read_csv(f'{BASE_DIR}/p1/MS-31_labels.csv')\n",
    "label_times.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_to_labels(1, prediction_dates = ['SMS'], churn_periods = [14],\n",
    "                    lead_times = [1], prediction_windows = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:41:28.313923Z",
     "start_time": "2018-10-31T16:40:41.333050Z"
    }
   },
   "outputs": [],
   "source": [
    "label_times = pd.read_csv(f'{BASE_DIR}/p1/SMS-14_labels.csv')\n",
    "label_times.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark for Parallelization\n",
    "\n",
    "The below code uses Spark to parallelize the label making. This particular implementation uses a single machine local cluster although the same idea can be extended to a cluster of machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T16:41:30.905484Z",
     "start_time": "2018-10-31T16:41:28.486308Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update accordingly based on your Spark setup\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# Enable logging\n",
    "conf.set('spark.eventLog.enabled', True);\n",
    "conf.set('spark.eventLog.dir', './data/tmp/');\n",
    "\n",
    "# Use all cores on a single machine\n",
    "conf.set('spark.num.executors', 1)\n",
    "conf.set('spark.executor.memory', '12g')\n",
    "conf.set('spark.executor.cores', 12)\n",
    "\n",
    "# Make sure to specify correct spark master ip\n",
    "sc = pyspark.SparkContext(master='spark://AMB-R09BLVCJ:7077',\n",
    "                          appName='labeling', conf=conf)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the large size of this data set, for this example we are only generating labels for the first 50 partitions of our data. If you are running this on a larger cluster and storing the results to S3 instead of local disk, you can run this on all 1000 partitions that have been created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:04:05.142275Z",
     "start_time": "2018-10-31T16:41:31.101028Z"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_PARTITIONS_TO_RUN = 50\n",
    "# Parallelize making labels in Spark\n",
    "start = timer()\n",
    "sc.parallelize(list(range(NUM_PARTITIONS_TO_RUN)), numSlices=NUM_PARTITIONS_TO_RUN).\\\n",
    "   map(partition_to_labels).collect()\n",
    "sc.stop()\n",
    "end = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Spark is running, you can navigate to localhost:4040 to see the details of the particular job, or to localhost:8081 to see the overview of the cluster. This is useful for diagnosing the state of a spark operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:04:05.338615Z",
     "start_time": "2018-10-31T20:04:05.336262Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{round(end - start)} seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:04:05.669969Z",
     "start_time": "2018-10-31T20:04:05.532570Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(f'{BASE_DIR}/p0/MS-31_labels.csv')\n",
    "labels.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T20:04:06.052876Z",
     "start_time": "2018-10-31T20:04:05.873216Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(f'{BASE_DIR}/p19/SMS-14_labels.csv')\n",
    "labels.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook, we implemented prediction engineering for the customer churn use case. After defining the business need, we translated it into a task that can be solved with machine learning and created a set of label times. We saw how to define functions with parameters so we could solve multiple prediction problems without needing to re-write the entire code. Although we only worked through two problems, there are numerous others that could be solved with the same data and approach.\n",
    "\n",
    "\n",
    "The label times contain cutoff times for a specific prediction problem along with the associated label. The label times can now be used to make features for each label by filtering the data to before the cutoff time. This ensures that any features made are valid and will automatically be taken care of in Featuretools. \n",
    "\n",
    "The general procedure for making labels is:\n",
    "\n",
    "1. Define the business requirement: predict customers who will churn during a specified period of time\n",
    "2. Translate the business requirement into a machine learning problem: given historical customer data, build a model to predict which customers will churn depending on several parameters\n",
    "3. Make labels along with cutoff times corresponding to the machine learning problem: develop functions that take in parameters so the same function can be used for multiple prediction problems.\n",
    "4. Label all past historical data: parallelize operations by partitioning data into independent subsets\n",
    "\n",
    "This approach can be extended to other problems. Although the exact syntax is specific to this use case, the overall approach is designed to be general purpose.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "With a complete set of label times, we can now make features for each label using the cutoff times to ensure our features are valid. However, instead of the painstaking and error-prone process of making features by hand, we can use automated feature engineering in [Featuretools](https://github.com/Featuretools/featuretools) to automated this process. Featuretools will build hundreds of relevant features using only a few lines of code and will automatically filter the data to ensure that all of our features are valid. The feature engineering pipeline is developed in the `Feature Engineering` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "501px",
    "left": "781px",
    "right": "20px",
    "top": "574px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
